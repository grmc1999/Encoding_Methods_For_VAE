{
      "experiment_state": "waiting",
      "optimizer": {
            "lr": 0.0001,
            "capturable": false
      },
      "trainer": {
            "model": null,
            "epochs": 200,
            "folds": 2,
            "batch_size": 2048,
            "use_cuda": "cuda:0",
            "data_dir": null,
            "in_device": "cuda:0",
            "num_workers": 10,
            "args": [
                  "x"
            ]
      },
      "transforms": {
            "Tuple_to_dict": {},
            "MultiInputToTensor": {
                  "metadata": []
            }
      },
      "model": {
            "model_class": "pytorch_VAE",
            "model_name": "Unpaired_Flexible_Encoding_Decoding_VAE",
            "sub_modules": {
                  "encoding_decoding_module": {
                        "module_type": "Asymmetrical_CNN_DNN_EDM",
                        "parameters": {
                              "encoder_parameters": {
                                    "image_shape": [
                                          28,
                                          28,
                                          1
                                    ],
                                    "patch_shape": [
                                          7,
                                          7
                                    ],
                                    "Transformer_layers_sizes": [
                                          132,
                                          164,
                                          196
                                    ],
                                    "Attention_mechanisms": [
                                          "Dot_product_attention",
                                          "Dot_product_attention",
                                          "Dot_product_attention"
                                    ],
                                    "layers_heads": [
                                          4,
                                          4,
                                          4
                                    ],
                                    "layer_head_sizes": [
                                          100,
                                          132,
                                          164
                                    ],
                                    "dropout": 0.5,
                                    "batch_norm": false,
                                    "pre_batch_norm": false,
                                    "pre_activator": {
                                          "name": "LeakyReLU",
                                          "params": {}
                                    },
                                    "pre_dropout": 0.5
                              },
                              "decoder_parameters": {
                                    "repr_sizes": [
                                          4,
                                          4,
                                          4,
                                          4,
                                          1
                                    ],
                                    "kernel_size": 9,
                                    "activators": {
                                          "name": [
                                                "LeakyReLU",
                                                "LeakyReLU",
                                                "LeakyReLU",
                                                "Sigmoid"
                                          ],
                                          "params": [
                                                {},
                                                {},
                                                {},
                                                {}
                                          ]
                                    },
                                    "pooling": false,
                                    "batch_norm": false,
                                    "dropout": null,
                                    "stride": [
                                          4,
                                          4,
                                          1,
                                          1,
                                          1
                                    ]
                              },
                              "flat": false,
                              "deflat": true,
                              "compression_factor": 0.25,
                              "i_shape": [
                                    28,
                                    28
                              ],
                              "Enc_type": "ViT_ENC",
                              "Dec_type": "CNN_DEC"
                        }
                  },
                  "P_NET": {
                        "variational_generation_type": "VAE_NETs",
                        "parameters": {
                              "layer_size": [
                                    196,
                                    196
                              ],
                              "dec_activators": {
                                    "name": "LeakyReLU",
                                    "params": {}
                              },
                              "batch_norm": false,
                              "dropout": null
                        }
                  },
                  "Q_NET": {
                        "variational_generation_type": "VAE_NETs",
                        "parameters": {
                              "layer_size": [
                                    196,
                                    196
                              ],
                              "enc_activators": {
                                    "name": "LeakyReLU",
                                    "params": {}
                              },
                              "batch_norm": false,
                              "dropout": null
                        }
                  }
            },
            "model_params": {
                  "losses_weigths": {
                        "reconstructive": 0.5,
                        "generative": 0.5
                  },
                  "resize": [
                        28,
                        28
                  ]
            }
      },
      "error": "Given groups=1, weight of size [32, 3, 5, 5], expected input[2048, 1, 28, 28] to have 3 channels, but got 1 channels instead"
}